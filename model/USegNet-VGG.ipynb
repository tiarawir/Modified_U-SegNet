{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4b497df-ec8d-4c6c-9abe-a4548f1d1fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda, Activation\n",
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15bf589d-b2c7-4ed6-8913-66ee94315233",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Load data'''\n",
    "sys.path.extend(['..'])\n",
    "from data.adv_data_generator import ImageLoader\n",
    "\n",
    "file = [\"train\"]\n",
    "type_mod = [\"img\",\"mask\",\"aug\"]\n",
    "dataset_aug = [[],[],[]]\n",
    "num = 0\n",
    "for i in file:\n",
    "    for j in type_mod:\n",
    "        loader = ImageLoader(i, j)\n",
    "        dataset_aug[num] = loader.loadAug()\n",
    "        num += 1\n",
    "\n",
    "file = [\"test\",\"val\"]\n",
    "type_ori = [\"images\",\"masks\"]\n",
    "dataset = [[],[],[],[]]\n",
    "num = 0\n",
    "for i in file:\n",
    "    for j in type_ori:\n",
    "        loader = ImageLoader(i, j)\n",
    "        dataset[num] = loader.loadData()\n",
    "        num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af707fd1-fc95-4ed9-8616-d2d1c0d7acb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Define dataset'''\n",
    "train_img = dataset_aug[0]\n",
    "train_mask = dataset_aug[1]\n",
    "train_aug = dataset_aug[2]\n",
    "\n",
    "test_img = dataset[0]\n",
    "test_mask = dataset[1]\n",
    "val_img = dataset[2]\n",
    "val_mask = dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ccd712e-8a12-4919-8dd0-34087ba2378f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6415, 64, 64)\n",
      "(6415, 64, 64)\n",
      "[0 1 2 3 4 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "'''\n",
    "(1) Set the seed\n",
    "(2) Generate a random permutation of indices\n",
    "(3) Use the same random indices for sampling both arrays\n",
    "'''\n",
    "\n",
    "def sampling(img, mask, seed, percentage=0.2):\n",
    "    sample_size = int(len(img) * percentage)\n",
    "    random.seed(seed)\n",
    "    random_indices = random.sample(range(len(img)), sample_size)\n",
    "    sample_img = img[random_indices]\n",
    "    sample_mask = mask[random_indices]\n",
    "\n",
    "    return sample_img, sample_mask\n",
    "\n",
    "sam_val_img, sam_val_mask = sampling(val_img, val_mask, 4)\n",
    "sam_test_img, sam_test_mask = sampling(test_img, test_mask, 7)\n",
    "\n",
    "print(sam_val_mask.shape)\n",
    "print(sam_test_mask.shape)\n",
    "print(np.unique(sam_test_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea85d9ba-a1d1-4160-a685-4dd7ab85fce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Define variable to run the model'''\n",
    "# X = np.concatenate((mod_train_img, mod_train_aug), axis=0)\n",
    "# y = np.concatenate((mod_train_mask, mod_train_mask), axis=0)\n",
    "X = train_img\n",
    "y = train_mask\n",
    "X_val = sam_val_img\n",
    "y_val = sam_val_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c13d7f1a-98fe-4eec-aed9-ce12c782fb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 8\n",
    "from keras.utils import to_categorical\n",
    "y_cat = to_categorical(y, num_classes=n_classes)\n",
    "y_val_cat = to_categorical(y_val, num_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "045202a9-ab41-40f3-9c1f-055d10a2de91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolingWithArgmax2D(Layer):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            pool_size=(2, 2),\n",
    "            strides=(2, 2),\n",
    "            padding='same',\n",
    "            **kwargs):\n",
    "        super(MaxPoolingWithArgmax2D, self).__init__(**kwargs)\n",
    "        self.padding = padding\n",
    "        self.pool_size = pool_size\n",
    "        self.strides = strides\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "#         print(\"max pooling with argmax\")\n",
    "        padding = self.padding\n",
    "        pool_size = self.pool_size\n",
    "        strides = self.strides\n",
    "        if K.backend() == 'tensorflow':\n",
    "            ksize = [1, pool_size[0], pool_size[1], 1]\n",
    "            padding = padding.upper()\n",
    "            strides = [1, strides[0], strides[1], 1]\n",
    "            output, argmax = tf.nn.max_pool_with_argmax(\n",
    "                    inputs,\n",
    "                    ksize=ksize,\n",
    "                    strides=strides,\n",
    "                    padding=padding)\n",
    "        else:\n",
    "            errmsg = '{} backend is not supported for layer {}'.format(\n",
    "                    K.backend(), type(self).__name__)\n",
    "            raise NotImplementedError(errmsg)\n",
    "        argmax = tf.cast(argmax, K.floatx())\n",
    "        return [output, argmax]\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "#         print(\"i guess its subsampling\")\n",
    "        ratio = (1, 2, 2, 1)\n",
    "        output_shape = [\n",
    "                dim//ratio[idx]\n",
    "                if dim is not None else None\n",
    "                for idx, dim in enumerate(input_shape)]\n",
    "        output_shape = tuple(output_shape)\n",
    "        return [output_shape, output_shape]\n",
    "\n",
    "class MaxUnpooling2D(Layer):\n",
    "    def __init__(self, size=(2, 2), **kwargs):\n",
    "        super(MaxUnpooling2D, self).__init__(**kwargs)\n",
    "        self.size = size\n",
    "\n",
    "    def call(self, inputs, output_shape=None):\n",
    "        # one is pool and one is mask\n",
    "        updates, mask = inputs[0], inputs[1]\n",
    "\n",
    "        mask = tf.cast(mask, 'int32')\n",
    "        input_shape = tf.shape(updates, out_type='int32')\n",
    "\n",
    "        #  calculation new shape\n",
    "        if output_shape is None:\n",
    "            output_shape = (\n",
    "                input_shape[0],\n",
    "                input_shape[1]*self.size[0],\n",
    "                input_shape[2]*self.size[1],\n",
    "                input_shape[3])\n",
    "        self.output_shape1 = output_shape\n",
    "\n",
    "        # calculation indices for batch, height, width and feature maps\n",
    "        one_like_mask = tf.ones_like(mask, dtype='int32')      #creates ones of the same shape as the mask\n",
    "        batch_shape = tf.concat([[input_shape[0]], [1], [1], [1]], axis=0)\n",
    "        batch_range = tf.reshape(tf.range(output_shape[0], dtype='int32'),shape=batch_shape)\n",
    "        b = one_like_mask * batch_range\n",
    "\n",
    "        y = mask // (output_shape[2] * output_shape[3])\n",
    "\n",
    "        x = (mask // output_shape[3]) % output_shape[2]\n",
    "\n",
    "        feature_range = tf.range(output_shape[3], dtype='int32')\n",
    "\n",
    "        f = one_like_mask * feature_range\n",
    "\n",
    "        # transpose indices & reshape update values to one dimension\n",
    "        updates_size = tf.size(updates)       # Prints the number of elements in the updates\n",
    "        indices = tf.transpose(tf.reshape(tf.stack([b, y, x, f]), [4, updates_size]))\n",
    "        values = tf.reshape(updates, [updates_size])\n",
    "        ret = tf.scatter_nd(indices, values, output_shape)\n",
    "        return ret\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        mask_shape = input_shape[1]\n",
    "        return (\n",
    "                mask_shape[0],\n",
    "                mask_shape[1]*self.size[0],\n",
    "                mask_shape[2]*self.size[1],\n",
    "                mask_shape[3]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3606007e-ad7c-42e9-965d-1e4fde0d0c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "import tensorflow as tf\n",
    "\n",
    "IMAGE_ORDERING = \"channels_last\"\n",
    "\n",
    "if IMAGE_ORDERING == 'channels_first':\n",
    "    pretrained_url = \"https://github.com/fchollet/deep-learning-models/\" \\\n",
    "                     \"releases/download/v0.1/\" \\\n",
    "                     \"vgg16_weights_th_dim_ordering_th_kernels_notop.h5\"\n",
    "elif IMAGE_ORDERING == 'channels_last':\n",
    "    pretrained_url = \"https://github.com/fchollet/deep-learning-models/\" \\\n",
    "                     \"releases/download/v0.1/\" \\\n",
    "                     \"vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "\n",
    "\n",
    "def get_vgg_encoder(input_height=224,  input_width=224, pretrained='imagenet', channels=3):\n",
    "\n",
    "    assert input_height % 32 == 0\n",
    "    assert input_width % 32 == 0\n",
    "\n",
    "    if IMAGE_ORDERING == 'channels_first':\n",
    "        img_input = Input(shape=(channels, input_height, input_width))\n",
    "    elif IMAGE_ORDERING == 'channels_last':\n",
    "        img_input = Input(shape=(input_height, input_width, channels))\n",
    "\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same',\n",
    "               name='block1_conv1', data_format=IMAGE_ORDERING)(img_input)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same',\n",
    "               name='block1_conv2', data_format=IMAGE_ORDERING)(x)\n",
    "    skip = x\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool',\n",
    "                     data_format=IMAGE_ORDERING)(x)\n",
    "    f1 = x\n",
    "    # Block 2\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same',\n",
    "               name='block2_conv1', data_format=IMAGE_ORDERING)(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same',\n",
    "               name='block2_conv2', data_format=IMAGE_ORDERING)(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool',\n",
    "                     data_format=IMAGE_ORDERING)(x)\n",
    "    f2 = x\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same',\n",
    "               name='block3_conv1', data_format=IMAGE_ORDERING)(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same',\n",
    "               name='block3_conv2', data_format=IMAGE_ORDERING)(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same',\n",
    "               name='block3_conv3', data_format=IMAGE_ORDERING)(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool',\n",
    "                     data_format=IMAGE_ORDERING)(x)\n",
    "    f3 = x\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n",
    "               name='block4_conv1', data_format=IMAGE_ORDERING)(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n",
    "               name='block4_conv2', data_format=IMAGE_ORDERING)(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n",
    "               name='block4_conv3', data_format=IMAGE_ORDERING)(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool',\n",
    "                     data_format=IMAGE_ORDERING)(x)\n",
    "    f4 = x\n",
    "\n",
    "    # Block 5\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n",
    "               name='block5_conv1', data_format=IMAGE_ORDERING)(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n",
    "               name='block5_conv2', data_format=IMAGE_ORDERING)(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n",
    "               name='block5_conv3', data_format=IMAGE_ORDERING)(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool',\n",
    "                     data_format=IMAGE_ORDERING)(x)\n",
    "    f5 = x\n",
    "\n",
    "    if pretrained == 'imagenet':\n",
    "        VGG_Weights_path = tf.keras.utils.get_file(\n",
    "            pretrained_url.split(\"/\")[-1], pretrained_url)\n",
    "        Model(img_input, x).load_weights(VGG_Weights_path, by_name=True, skip_mismatch=True)\n",
    "\n",
    "    return img_input, [f1, f2, f3, f4, f5], skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12c7a653-148e-47c1-9b3b-da59dbb049eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = 'relu'\n",
    "\n",
    "def Conv2DBlock(input_tensor, filters, kernel_size, name_block):\n",
    "    x = layers.Conv2D(filters, kernel_size, padding='same', name=name_block)(input_tensor)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(activation)(x)\n",
    "    return x\n",
    "    \n",
    "def usegnet_vgg(n_classes, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS, pool_size=(2, 2)):\n",
    "\n",
    "    img_input, [f1, f2, f3, f4, f5], skip = get_vgg_encoder(input_height=IMG_HEIGHT, input_width=IMG_WIDTH)\n",
    "    \n",
    "    #bottleneck\n",
    "    x = Conv2DBlock(f3, 256, 3, \"block4_conv1\")\n",
    "    x = Conv2DBlock(x, 256, 3, \"block4_conv2\")\n",
    "    x = Conv2DBlock(x, 256, 3, \"block4_conv3\")\n",
    "    \n",
    "    #decoder\n",
    "    unpool_1 = UpSampling2D((2, 2), name=\"block5_unpool\")(x)\n",
    "    x = Conv2DBlock(unpool_1, 256, 3, \"block5_conv1\")\n",
    "    x = Conv2DBlock(x, 256, 3, \"block5_conv2\")\n",
    "    x = Conv2DBlock(x, 128, 3, \"block5_conv3\")\n",
    "    \n",
    "    unpool_2 = UpSampling2D((2, 2), name=\"block6_unpool\")(x)\n",
    "    x = Conv2DBlock(unpool_2, 128, 3, \"block6_conv1\")\n",
    "    x = Conv2DBlock(x, 64, 3, \"block6_conv2\")\n",
    "    \n",
    "    unpool_3 = UpSampling2D((2, 2), name=\"block7_unpool\")(x)\n",
    "    x = concatenate([unpool_3, skip], axis = -1, name=\"block7_concatenate\")\n",
    "    x = Conv2D(64, (1, 1), padding=\"same\", kernel_initializer='he_normal', name=\"block7_conv1\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    \n",
    "    x = Conv2DBlock(x, 64, 3, \"block8_conv1\")\n",
    "    x = Conv2D(n_classes, (1, 1), padding=\"same\", kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    outputs = Activation(\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=[img_input], outputs=[outputs])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a69fb192-6752-4a08-959d-e57f1cf752dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the weighted categorical cross-entropy loss function\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = tf.constant(weights, dtype=tf.float32)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= tf.reduce_sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * tf.math.log(y_pred) * weights\n",
    "        loss = -tf.reduce_sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be9b6a50-17c4-4475-912e-be464277c29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = X.shape[1]\n",
    "IMG_WIDTH  = X.shape[2]\n",
    "IMG_CHANNELS = X.shape[3]\n",
    "weights = np.array([1914880.0, 11.11, 9.98, 2.2, 1.14, 0.44, 0.25, 2.92])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d04bfb16-6994-4c67-bf8a-fc05db0792e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = usegnet_vgg(8, 64, 64, 3)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccf385db-3b83-45be-a882-59884427194f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `tf.keras` framework.\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "import segmentation_models as sm\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=['accuracy', sm.metrics.iou_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92843bcd-1f99-460b-8da3-55cce8547508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1074s\u001b[0m 2s/step - accuracy: 0.5539 - iou_score: 0.1005 - loss: 1.4767 - val_accuracy: 0.6635 - val_iou_score: 0.1488 - val_loss: 1.0737\n",
      "Epoch 2/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1066s\u001b[0m 2s/step - accuracy: 0.6885 - iou_score: 0.1655 - loss: 0.9516 - val_accuracy: 0.6330 - val_iou_score: 0.1716 - val_loss: 1.0308\n",
      "Epoch 3/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1067s\u001b[0m 2s/step - accuracy: 0.7058 - iou_score: 0.1951 - loss: 0.8125 - val_accuracy: 0.7261 - val_iou_score: 0.2142 - val_loss: 0.7477\n",
      "Epoch 4/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1066s\u001b[0m 2s/step - accuracy: 0.7254 - iou_score: 0.2187 - loss: 0.7242 - val_accuracy: 0.4615 - val_iou_score: 0.1514 - val_loss: 1.3914\n",
      "Epoch 5/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1067s\u001b[0m 2s/step - accuracy: 0.7377 - iou_score: 0.2349 - loss: 0.6738 - val_accuracy: 0.7075 - val_iou_score: 0.2280 - val_loss: 0.7470\n",
      "Epoch 6/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1067s\u001b[0m 2s/step - accuracy: 0.7457 - iou_score: 0.2465 - loss: 0.6412 - val_accuracy: 0.7540 - val_iou_score: 0.2555 - val_loss: 0.6199\n",
      "Epoch 7/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1067s\u001b[0m 2s/step - accuracy: 0.7545 - iou_score: 0.2580 - loss: 0.6096 - val_accuracy: 0.7443 - val_iou_score: 0.2593 - val_loss: 0.6359\n",
      "Epoch 8/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1067s\u001b[0m 2s/step - accuracy: 0.7635 - iou_score: 0.2693 - loss: 0.5816 - val_accuracy: 0.7409 - val_iou_score: 0.2649 - val_loss: 0.6421\n",
      "Epoch 9/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1067s\u001b[0m 2s/step - accuracy: 0.7714 - iou_score: 0.2789 - loss: 0.5585 - val_accuracy: 0.7275 - val_iou_score: 0.2611 - val_loss: 0.6706\n",
      "Epoch 10/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1067s\u001b[0m 2s/step - accuracy: 0.7779 - iou_score: 0.2870 - loss: 0.5398 - val_accuracy: 0.7496 - val_iou_score: 0.2748 - val_loss: 0.6079\n",
      "Epoch 11/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1067s\u001b[0m 2s/step - accuracy: 0.7842 - iou_score: 0.2943 - loss: 0.5227 - val_accuracy: 0.7334 - val_iou_score: 0.2725 - val_loss: 0.6496\n",
      "Epoch 12/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1067s\u001b[0m 2s/step - accuracy: 0.7913 - iou_score: 0.3023 - loss: 0.5045 - val_accuracy: 0.7467 - val_iou_score: 0.2818 - val_loss: 0.6132\n",
      "Epoch 13/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1068s\u001b[0m 2s/step - accuracy: 0.7964 - iou_score: 0.3085 - loss: 0.4918 - val_accuracy: 0.7611 - val_iou_score: 0.2897 - val_loss: 0.5838\n",
      "Epoch 14/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1069s\u001b[0m 2s/step - accuracy: 0.8025 - iou_score: 0.3156 - loss: 0.4760 - val_accuracy: 0.7161 - val_iou_score: 0.2721 - val_loss: 0.7002\n",
      "Epoch 15/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1068s\u001b[0m 2s/step - accuracy: 0.8068 - iou_score: 0.3205 - loss: 0.4653 - val_accuracy: 0.7671 - val_iou_score: 0.3027 - val_loss: 0.5787\n",
      "Epoch 16/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1068s\u001b[0m 2s/step - accuracy: 0.8142 - iou_score: 0.3289 - loss: 0.4468 - val_accuracy: 0.7516 - val_iou_score: 0.3032 - val_loss: 0.6183\n",
      "Epoch 17/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1068s\u001b[0m 2s/step - accuracy: 0.8191 - iou_score: 0.3343 - loss: 0.4353 - val_accuracy: 0.7640 - val_iou_score: 0.3075 - val_loss: 0.5915\n",
      "Epoch 18/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1068s\u001b[0m 2s/step - accuracy: 0.8229 - iou_score: 0.3389 - loss: 0.4259 - val_accuracy: 0.6959 - val_iou_score: 0.2779 - val_loss: 0.8119\n",
      "Epoch 19/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1069s\u001b[0m 2s/step - accuracy: 0.8266 - iou_score: 0.3440 - loss: 0.4163 - val_accuracy: 0.6439 - val_iou_score: 0.2459 - val_loss: 1.0024\n",
      "Epoch 20/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1069s\u001b[0m 2s/step - accuracy: 0.8284 - iou_score: 0.3472 - loss: 0.4119 - val_accuracy: 0.7297 - val_iou_score: 0.2918 - val_loss: 0.7079\n",
      "Epoch 21/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1068s\u001b[0m 2s/step - accuracy: 0.8294 - iou_score: 0.3489 - loss: 0.4090 - val_accuracy: 0.7162 - val_iou_score: 0.2827 - val_loss: 0.7775\n",
      "Epoch 22/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1068s\u001b[0m 2s/step - accuracy: 0.8313 - iou_score: 0.3518 - loss: 0.4040 - val_accuracy: 0.7276 - val_iou_score: 0.2961 - val_loss: 0.7372\n",
      "Epoch 23/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1069s\u001b[0m 2s/step - accuracy: 0.8339 - iou_score: 0.3553 - loss: 0.3976 - val_accuracy: 0.7451 - val_iou_score: 0.2995 - val_loss: 0.6945\n",
      "Epoch 24/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1069s\u001b[0m 2s/step - accuracy: 0.8336 - iou_score: 0.3559 - loss: 0.3975 - val_accuracy: 0.7497 - val_iou_score: 0.3111 - val_loss: 0.6726\n",
      "Epoch 25/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1069s\u001b[0m 2s/step - accuracy: 0.8363 - iou_score: 0.3594 - loss: 0.3912 - val_accuracy: 0.7382 - val_iou_score: 0.2982 - val_loss: 0.7347\n",
      "Epoch 26/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1069s\u001b[0m 2s/step - accuracy: 0.8405 - iou_score: 0.3646 - loss: 0.3815 - val_accuracy: 0.7324 - val_iou_score: 0.3013 - val_loss: 0.7344\n",
      "Epoch 27/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1069s\u001b[0m 2s/step - accuracy: 0.8391 - iou_score: 0.3644 - loss: 0.3841 - val_accuracy: 0.7718 - val_iou_score: 0.3256 - val_loss: 0.6047\n",
      "Epoch 28/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1069s\u001b[0m 2s/step - accuracy: 0.8410 - iou_score: 0.3671 - loss: 0.3792 - val_accuracy: 0.7833 - val_iou_score: 0.3325 - val_loss: 0.5828\n",
      "Epoch 29/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1069s\u001b[0m 2s/step - accuracy: 0.8459 - iou_score: 0.3733 - loss: 0.3675 - val_accuracy: 0.7754 - val_iou_score: 0.3312 - val_loss: 0.6072\n",
      "Epoch 30/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1070s\u001b[0m 2s/step - accuracy: 0.8488 - iou_score: 0.3775 - loss: 0.3608 - val_accuracy: 0.7599 - val_iou_score: 0.3203 - val_loss: 0.6624\n",
      "Epoch 31/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1069s\u001b[0m 2s/step - accuracy: 0.8506 - iou_score: 0.3798 - loss: 0.3564 - val_accuracy: 0.7540 - val_iou_score: 0.3214 - val_loss: 0.6752\n",
      "Epoch 32/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1070s\u001b[0m 2s/step - accuracy: 0.8513 - iou_score: 0.3816 - loss: 0.3545 - val_accuracy: 0.7675 - val_iou_score: 0.3184 - val_loss: 0.6928\n",
      "Epoch 33/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1070s\u001b[0m 2s/step - accuracy: 0.8512 - iou_score: 0.3822 - loss: 0.3552 - val_accuracy: 0.7668 - val_iou_score: 0.3137 - val_loss: 0.7216\n",
      "Epoch 34/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1070s\u001b[0m 2s/step - accuracy: 0.8510 - iou_score: 0.3830 - loss: 0.3546 - val_accuracy: 0.7708 - val_iou_score: 0.3172 - val_loss: 0.6969\n",
      "Epoch 35/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1070s\u001b[0m 2s/step - accuracy: 0.8532 - iou_score: 0.3861 - loss: 0.3499 - val_accuracy: 0.7293 - val_iou_score: 0.2762 - val_loss: 0.9109\n",
      "Epoch 36/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1069s\u001b[0m 2s/step - accuracy: 0.8546 - iou_score: 0.3888 - loss: 0.3465 - val_accuracy: 0.7365 - val_iou_score: 0.2817 - val_loss: 0.9201\n",
      "Epoch 37/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1070s\u001b[0m 2s/step - accuracy: 0.8524 - iou_score: 0.3872 - loss: 0.3519 - val_accuracy: 0.7687 - val_iou_score: 0.3151 - val_loss: 0.7235\n",
      "Epoch 38/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1071s\u001b[0m 2s/step - accuracy: 0.8555 - iou_score: 0.3913 - loss: 0.3452 - val_accuracy: 0.7768 - val_iou_score: 0.3322 - val_loss: 0.6163\n",
      "Epoch 39/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1071s\u001b[0m 2s/step - accuracy: 0.8586 - iou_score: 0.3951 - loss: 0.3379 - val_accuracy: 0.7619 - val_iou_score: 0.3351 - val_loss: 0.6627\n",
      "Epoch 40/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1070s\u001b[0m 2s/step - accuracy: 0.8602 - iou_score: 0.3978 - loss: 0.3338 - val_accuracy: 0.6845 - val_iou_score: 0.2963 - val_loss: 0.9292\n",
      "Epoch 41/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1070s\u001b[0m 2s/step - accuracy: 0.8610 - iou_score: 0.3991 - loss: 0.3315 - val_accuracy: 0.7119 - val_iou_score: 0.3081 - val_loss: 0.8300\n",
      "Epoch 42/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1071s\u001b[0m 2s/step - accuracy: 0.8625 - iou_score: 0.4026 - loss: 0.3273 - val_accuracy: 0.7542 - val_iou_score: 0.3334 - val_loss: 0.6864\n",
      "Epoch 43/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1070s\u001b[0m 2s/step - accuracy: 0.8652 - iou_score: 0.4057 - loss: 0.3216 - val_accuracy: 0.7299 - val_iou_score: 0.3205 - val_loss: 0.7906\n",
      "Epoch 44/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1070s\u001b[0m 2s/step - accuracy: 0.8687 - iou_score: 0.4109 - loss: 0.3133 - val_accuracy: 0.7559 - val_iou_score: 0.3331 - val_loss: 0.6863\n",
      "Epoch 45/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1070s\u001b[0m 2s/step - accuracy: 0.8694 - iou_score: 0.4130 - loss: 0.3118 - val_accuracy: 0.7657 - val_iou_score: 0.3368 - val_loss: 0.6642\n",
      "Epoch 46/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1071s\u001b[0m 2s/step - accuracy: 0.8699 - iou_score: 0.4143 - loss: 0.3103 - val_accuracy: 0.7559 - val_iou_score: 0.3412 - val_loss: 0.6950\n",
      "Epoch 47/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1073s\u001b[0m 2s/step - accuracy: 0.8712 - iou_score: 0.4160 - loss: 0.3078 - val_accuracy: 0.7643 - val_iou_score: 0.3421 - val_loss: 0.6736\n",
      "Epoch 48/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1070s\u001b[0m 2s/step - accuracy: 0.8696 - iou_score: 0.4145 - loss: 0.3106 - val_accuracy: 0.7558 - val_iou_score: 0.3307 - val_loss: 0.7474\n",
      "Epoch 49/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1071s\u001b[0m 2s/step - accuracy: 0.8717 - iou_score: 0.4173 - loss: 0.3060 - val_accuracy: 0.7614 - val_iou_score: 0.3303 - val_loss: 0.7319\n",
      "Epoch 50/50\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1071s\u001b[0m 2s/step - accuracy: 0.8739 - iou_score: 0.4210 - loss: 0.3008 - val_accuracy: 0.7540 - val_iou_score: 0.3278 - val_loss: 0.7485\n",
      "Training time: 53456.7973 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "history = model.fit(X, y_cat, \n",
    "                    batch_size = 64, \n",
    "                    verbose=1, \n",
    "                    epochs=50, \n",
    "                    validation_data=(X_val, y_val_cat),\n",
    "                    shuffle=False)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Training time: {training_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ef37a37-5166-490c-b65c-dd61d2ebda40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = sam_test_img, sam_test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab2a9b68-5291-4748-9a1f-39b360bb0100",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_cat = to_categorical(y_test, num_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0b6d171-08d0-4cf9-a949-76b00af04f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 273ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=-1)\n",
    "y_true_classes = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a16deb1d-46a4-4a09-9e6f-c0ef2d913964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the arrays\n",
    "y_pred_flat = y_pred_classes.flatten()\n",
    "y_true_flat = y_true_classes.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "758cefe7-e3a9-48b9-968c-4e932627a1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kholid\\anaconda3\\envs\\tir\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\kholid\\anaconda3\\envs\\tir\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.00      0.00      0.00         8\n",
      "     Class 1       0.47      0.10      0.16    293759\n",
      "     Class 2       0.37      0.64      0.47    324962\n",
      "     Class 3       0.48      0.48      0.48   1487533\n",
      "     Class 4       0.60      0.61      0.60   2862958\n",
      "     Class 5       0.66      0.78      0.72   7413470\n",
      "     Class 6       0.95      0.82      0.88  12772966\n",
      "     Class 8       0.52      0.63      0.57   1120184\n",
      "\n",
      "    accuracy                           0.75  26275840\n",
      "   macro avg       0.51      0.51      0.49  26275840\n",
      "weighted avg       0.77      0.75      0.76  26275840\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kholid\\anaconda3\\envs\\tir\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Generate the classification report\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(y_true_flat, y_pred_flat, target_names=['Class 0', \n",
    "                                                                       'Class 1', \n",
    "                                                                       'Class 2', \n",
    "                                                                       'Class 3', \n",
    "                                                                       'Class 4', \n",
    "                                                                       'Class 5', \n",
    "                                                                       'Class 6', \n",
    "                                                                       'Class 8'])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df31ebf0-e567-4860-9eee-3c00efea1809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean IoU = 0.36115667\n"
     ]
    }
   ],
   "source": [
    "#IOU\n",
    "\n",
    "n_classes = 8\n",
    "IOU_keras = MeanIoU(num_classes=n_classes)  \n",
    "IOU_keras.update_state(y_test, y_pred_classes)\n",
    "print(\"Mean IoU =\", IOU_keras.result().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
